# Mini RAG Knowledge Base Chatbot (Local)

ì™„ì „ ë¡œì»¬ RAG ì±—ë´‡. Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ **API í‚¤ ì—†ì´** ë¬´ë£Œë¡œ ë™ì‘í•©ë‹ˆë‹¤.

## Features

- ğŸ  **100% Local**: ì™¸ë¶€ API ì—†ìŒ, ë°ì´í„°ê°€ ë¡œì»¬ì—ë§Œ ì €ì¥
- ğŸ†“ **ë¬´ë£Œ**: Ollama + ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ ì‚¬ìš©
- ğŸ³ **Docker ì§€ì›**: `docker-compose up` í•œ ì¤„ë¡œ í™˜ê²½ êµ¬ì„±
- ğŸ“š **Document Ingestion**: `.txt`, `.md` íŒŒì¼ ì§€ì‹ ë² ì´ìŠ¤í™”
- ğŸ” **Semantic Search**: ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰
- âš™ï¸ **ëª¨ë¸ ì„ íƒ ê°€ëŠ¥**: Qwen2.5, Llama3.1, Gemma3 ë“±

## Requirements

- Docker & Docker Compose (ê¶Œì¥)
- ë˜ëŠ” [Ollama](https://ollama.ai) ì§ì ‘ ì„¤ì¹˜
- RAM: ìµœì†Œ 8GB (16GB ê¶Œì¥)

## Quick Start

### 1. Start Ollama (Docker)

```bash
# Ollama ì»¨í…Œì´ë„ˆ ì‹œì‘
docker-compose up -d

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ)
docker exec -it ollama ollama pull qwen2.5:3b
docker exec -it ollama ollama pull nomic-embed-text
```

ë˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©:
```bash
chmod +x scripts/setup-models.sh
./scripts/setup-models.sh
```

### 2. Install dependencies

```bash
npm install
```

### 3. Configure (optional)

```bash
cp .env.example .env
# í•„ìš”ì‹œ ëª¨ë¸ ë³€ê²½
```

### 4. Add documents

`knowledge/` í´ë”ì— `.txt` ë˜ëŠ” `.md` íŒŒì¼ ì¶”ê°€

### 5. Ingest & Chat

```bash
# ë¬¸ì„œ ì„ë² ë”©
npm run ingest

# ì±—ë´‡ ì‹œì‘
npm start
```

## Available Models

### LLM (Chat)

| Model | Size | RAM | íŠ¹ì§• |
|-------|------|-----|------|
| `qwen2.5:3b` | ~2GB | 4GB+ | ë¹ ë¦„, í•œêµ­ì–´ OK |
| `qwen2.5:7b` | ~4GB | 8GB+ | ê· í˜• |
| `llama3.1:8b` | ~5GB | 8GB+ | ì˜ì–´ ìš°ìˆ˜ |
| `gemma3:4b` | ~3GB | 6GB+ | Google ëª¨ë¸ |
| `qwen2.5:14b` | ~8GB | 16GB+ | ê³ í’ˆì§ˆ |

### Embedding

| Model | Size | íŠ¹ì§• |
|-------|------|------|
| `nomic-embed-text` | ~275MB | ì¶”ì²œ, ë¹ ë¦„ |
| `mxbai-embed-large` | ~670MB | ê³ í’ˆì§ˆ |

## Project Structure

```
â”œâ”€â”€ index.js              # CLI entry point
â”œâ”€â”€ docker-compose.yml    # Ollama ì»¨í…Œì´ë„ˆ
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ setup-models.sh   # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ loader.js         # Document loading & chunking
â”‚   â”œâ”€â”€ embeddings.js     # Ollama API wrapper
â”‚   â”œâ”€â”€ vectorStore.js    # Vector store (cosine similarity)
â”‚   â””â”€â”€ rag.js            # RAG pipeline
â”œâ”€â”€ knowledge/            # Your documents
â”œâ”€â”€ data/                 # Vector storage
â””â”€â”€ .env.example
```

## Commands

```bash
npm start          # ì±—ë´‡ ì‹œì‘
npm run ingest     # ë¬¸ì„œ ì„ë² ë”©
npm run check      # Ollama ìƒíƒœ í™•ì¸
npm run docker:up  # Ollama ì»¨í…Œì´ë„ˆ ì‹œì‘
npm run docker:down # ì»¨í…Œì´ë„ˆ ì¤‘ì§€
```

### Chat Commands

- `/stats` - ì§€ì‹ ë² ì´ìŠ¤ í†µê³„
- `/sources` - ë¡œë“œëœ ë¬¸ì„œ ëª©ë¡
- `/check` - Ollama ì—°ê²° ìƒíƒœ
- `/quit` - ì¢…ë£Œ

## Configuration

`.env` íŒŒì¼:

| Variable | Default | Description |
|----------|---------|-------------|
| `OLLAMA_BASE_URL` | `http://localhost:11434` | Ollama ì„œë²„ ì£¼ì†Œ |
| `EMBEDDING_MODEL` | `nomic-embed-text` | ì„ë² ë”© ëª¨ë¸ |
| `CHAT_MODEL` | `qwen2.5:3b` | LLM ëª¨ë¸ |
| `CHUNK_SIZE` | `500` | ì²­í¬ í¬ê¸° |
| `TOP_K` | `3` | ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ |

## GPU Acceleration

NVIDIA GPU ì‚¬ìš© ì‹œ `docker-compose.yml`ì—ì„œ ì£¼ì„ í•´ì œ:

```yaml
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: all
          capabilities: [gpu]
```

## AnythingLLM (Optional)

ì›¹ UIê°€ í•„ìš”í•˜ë©´ `docker-compose.yml`ì—ì„œ AnythingLLM ì£¼ì„ í•´ì œ í›„:

```bash
docker-compose up -d
# http://localhost:3001 ì ‘ì†
```

## License

MIT

---

*Created with help from OpenClaw* âš¡
